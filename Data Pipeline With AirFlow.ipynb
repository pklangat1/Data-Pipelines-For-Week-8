{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.5 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Windows/py.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import the libraries needed\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='pipeline.log', level=logging.DEBUG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'JKibera Telecoms',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 4, 5),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dag = DAG('data_pipeline', default_args=default_args, schedule_interval='*/1 * * * *')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(**kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function reads csv files on local disk | converts the from dataframe to json containing dictionary | returns the data in json format\n",
    "    \"\"\"\n",
    "    \n",
    "    json_cust = None\n",
    "    json_order = None\n",
    "    json_pay = None\n",
    "    \n",
    "    try:\n",
    "        # extract data from CSV files\n",
    "        # load the CSV data into Pandas dataframes for later transformation\n",
    "        df_cust_data=pd.read_csv(\"customer_data.csv\")\n",
    "        df_order_data=pd.read_csv(\"order_data.csv\")\n",
    "        df_pay_data=pd.read_csv(\"payment_data.csv\")\n",
    "        \n",
    "        #convert df to dictionary\n",
    "        dict_cust = df_cust_data.to_dict()\n",
    "        dict_order = df_order_data.to_dict()\n",
    "        dict_pay = df_pay_data.to_dict()\n",
    "        \n",
    "        #serialize to json\n",
    "        json_cust = json.dumps(dict_cust)\n",
    "        json_order = json.dumps(dict_order)\n",
    "        json_pay = json.dumps(dict_pay)\n",
    "    \n",
    "    except Exception as e:\n",
    "        err = \"Extract() error - \"+ str(e)\n",
    "        logging.debug(err)\n",
    "    \n",
    "    \n",
    "    return json_cust,json_order,json_pay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(**kwargs): \n",
    "    \n",
    "    \"\"\"\n",
    "    Function transforms data | merges dataframes | drops unneeded columns | returns a json string containing dictionary of values\n",
    "    \"\"\"\n",
    "    \n",
    "    json_transform=None\n",
    "    \n",
    "    try:\n",
    "        jfc,jfo,jfp = kwargs['ti'].xcom_pull(task_ids='extract_data')\n",
    "        \n",
    "        json_dict_dfc = json.loads(jfc)\n",
    "        json_dict_dfo = json.loads(jfo)\n",
    "        json_dict_dfp = json.loads(jfp)\n",
    "        \n",
    "        dfc = pd.DataFrame.from_dict(json_dict_dfc)\n",
    "        dfo = pd.DataFrame.from_dict(json_dict_dfo)\n",
    "        dfp = pd.DataFrame.from_dict(json_dict_dfp)\n",
    "        \n",
    "        \n",
    "        # convert date fields to the correct format using pd.to_datetime\n",
    "        dfc['date_of_birth'] = pd.to_datetime(dfc['date_of_birth'])\n",
    "        dfo['order_date'] = pd.to_datetime(dfo['order_date'])\n",
    "        dfp['payment_date'] = pd.to_datetime(dfp['payment_date'])\n",
    "        \n",
    "        #add json serializable date str columns\n",
    "        dfc['date_of_birth_str'] = dfc['date_of_birth'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        dfo['order_date_str'] = dfo['order_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        dfp['payment_date_str'] = dfp['payment_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        # merge customer and order dataframes on the customer_id column\n",
    "        co_merge_df = pd.merge(dfc, dfo, on='customer_id')\n",
    "        \n",
    "        # merge payment dataframe with the merged dataframe on the order_id and customer_id columns\n",
    "        pay_merge_df = pd.merge(co_merge_df, dfp, on=['customer_id', 'order_id'])\n",
    "        \n",
    "        # drop unnecessary columns like customer_id and order_id\n",
    "        df = pay_merge_df.drop(['customer_id', 'order_id'], axis=1)\n",
    "        \n",
    "        # group the data by customer and aggregate the amount paid using sum\n",
    "        grouped_df = df.groupby('first_name').agg({'amount': 'sum'})\n",
    "        \n",
    "        # create a new column to calculate the total value of orders made by each customer\n",
    "        df['total_order_value'] = df.groupby('first_name')['amount'].transform('sum')\n",
    "        \n",
    "        # calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan) \n",
    "        # from visual inspection of merged_df - each customer has 1 order in 2023 - mean_order_value=sum_of_order, all orders made in 2023 ..hence lifespan =1yr\n",
    "        df['clv'] = df['amount'] * 1 * 1\n",
    "        \n",
    "        #df with serializable timestamp columns \n",
    "        dfs = df.drop(['date_of_birth', 'order_date', 'payment_date'], axis=1)\n",
    "        \n",
    "        #convert df to dictionary\n",
    "        dict_transform = dfs.to_dict()\n",
    "        \n",
    "        #serialize to json\n",
    "        json_transform = json.dumps(dict_transform)\n",
    "        \n",
    "    except Exception as e:\n",
    "        err = \"Transform() error - \"+ str(e)\n",
    "        logging.debug(err)\n",
    "\n",
    "    return json_transform\n",
    "    \n",
    "\n",
    "\n",
    "def load_data(**kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function connects to postgres instance running on docker desktop using pg_hook \n",
    "    Transformed_data is loaded into postgres database \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        transformed_data = kwargs['ti'].xcom_pull(task_ids='transform_data')\n",
    "        \n",
    "        transformed_data = json.loads(transformed_data)\n",
    "\n",
    "        # convert the data to a list of tuples\n",
    "        result = []\n",
    "        \n",
    "        for key, value in transformed_data[\"first_name\"].items():\n",
    "            first_name=value,\n",
    "            last_name=transformed_data[\"last_name\"][key],\n",
    "            email=transformed_data[\"email\"][key],\n",
    "            country=transformed_data[\"country\"][key],\n",
    "            gender=transformed_data[\"gender\"][key],\n",
    "            date_of_birth_str=transformed_data[\"date_of_birth_str\"][key],\n",
    "            product=transformed_data[\"product\"][key],\n",
    "            price=transformed_data[\"price\"][key],\n",
    "            order_date_str=transformed_data[\"order_date_str\"][key],\n",
    "            payment_id=transformed_data[\"payment_id\"][key],\n",
    "            amount=transformed_data[\"amount\"][key],\n",
    "            payment_date_str=transformed_data[\"payment_date_str\"][key],\n",
    "            total_order_value=transformed_data[\"total_order_value\"][key],\n",
    "            clv=transformed_data[\"clv\"][key]\n",
    "            result.append((first_name,last_name,email,country,gender,date_of_birth_str,product,price,order_date_str,payment_id,amount,payment_date_str,total_order_value,clv))\n",
    "        \n",
    "        columns=['first_name','last_name','email','country','gender','date_of_birth_str','product','price','order_date_str','payment_id','amount','payment_date_str','total_order_value','clv']\n",
    "        pg_hook = PostgresHook(postgres_conn_id='my_postgres_db')\n",
    "        pg_hook.insert_rows(table='customer_orders', rows=result, target_fields=columns)\n",
    "    \n",
    "    except Exception as e:\n",
    "        err = \"Load() error - \"+ str(e)\n",
    "        logging.debug(err)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with dag:\n",
    "    \n",
    "    # extract data \n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract_data',\n",
    "        python_callable=extract_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    # transform data \n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_data',\n",
    "        python_callable=transform_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    # load data \n",
    "    load_task = PythonOperator(\n",
    "        task_id='load_data',\n",
    "        python_callable=load_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "     # define dependencies extract_data >> transform_data >> load_data\n",
    "    extract_task >> transform_task >> load_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e3d2864960cda1cc89e7405ec595e77e7ac30692c1b4230c1dcf8d9a5036813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
